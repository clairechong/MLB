---
title: MLB
output: 
  html_document:
  df_print: paged
  code_folding: show
---
Set up libraries and working directory

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r }
library(readr)
library(ggplot2)
library(dplyr)

setwd("C:/Users/jpano/Documents/DACourse/CSDA1010/sprints/sprint_1")
```

Load files
```{r }
appearances=read_csv("Appearances.csv", col_name=TRUE,na=c("","NA","#NA"))
batting=read_csv("Batting.csv",col_name=TRUE,na=c("","NA","#NA"))
fielding=read_csv("Fielding.csv",col_name=TRUE,na=c("","NA","#NA"))
appear_bat=merge(appearances,batting,by=c("yearID","playerID","teamID"))
appear_bat_field=merge(appear_bat,fielding,by=c("yearID","playerID","teamID"))
```
Let's look at the merged dataset
```{r}
baseball=appear_bat_field
head(baseball)
```
PB (Passed Balls), WP (Wild Pitches), SB.y (Opponent Stolen Bases) and CS.y (Opponents Caught Stealing) are all stats related only to catchers, which represents a small portion of the entire data. Since we want to look at stats
that apply to all players, regardless of position, and since greater than 90% of the data is missing from these columns, we removed them.  ZR (Zone Rating) - Greater than 90% of the data is missing so we decided to remove this attribute. Also, this is not a commonly known baseball statistic so is likely not going to be useful to the model.
```{r}
baseball$PB=baseball$WP=baseball$SB.y=baseball$CS.y=baseball$ZR=NULL
```
Check for duplicated columns
```{r}
duplicated(t(baseball))
```
lgID.y and lgID are duplicate columns of lgID.x. These  duplicates were created when merging the original files. Therefore, we can remove them.
```{r}
baseball$lgID=baseball$lgID.y=NULL
```
Data prior to 1960 was removed since the rules were different in the earlier years of baseball. The current leagues had not yet been formed; games were low scoring during certain years and then high scoring in other years. The post-war era lasted until 19601 so we decided to analyze data from that point on while still having a large amount of data at our disposal. We created a subset of the data in R for 1960-2016 called "baseball_sub".
```{r}
names(baseball)
baseball_sub <- subset(baseball, yearID >=1960)
head(baseball_sub)
```
Pitchers can have a great career even if they are not good hitters because their primary focus is on pitching, not hitting. Pitchers generally have very few at-nats compared to other position players. Since we're focused on hitting stats we will remove pitchers from the dataset as keeping them in would skew the data.
```{r}
baseball_sub2 = subset(baseball_sub, POS != "P")
nrow(baseball_sub2)
ncol(baseball_sub2)
```
Calculate batting average and put it in a new column
```{r}
baseball_sub2$BA=baseball_sub2$H/baseball_sub2$AB
head(baseball_sub2$BA)
```
Identify missing values
```{r}
missv=function(x){sum(is.na(x))/length(x)*100}
apply(baseball_sub2,2,missv)
```
Games Started-Fielding (GS.y) and Time Played in the Field Expressed as Outs (InnOuts) has some NAs due to some blank values. These attributes have zeros in other instances when the player did not start the game (GS.y) or when the
player did not play long enough for an out to be recorded (InnOuts) so we do not want to assume that a blank is the same as zero. As a result, we decided to leave the NAs in for now, especially since 17% of the instances in both cases have blanks (not an insignificant amount).
```{r}

```
Batting Average (BA) has NAs because there are players that did not have any At Bats, so the calculation would be dividing by zero which yields NA. We want to analyze offensive performance of players who have plate appearances, so those that don't should not be included in the dataset. Since BA is ratio data, zero means something (it's not arbitrary). Imputing to zero skews the results towards zero, and we don't want to do that, so instead we remove instances where BA=N/A.
```{r}
baseball_sub2=baseball_sub2[!is.na(baseball_sub2$BA),]
```

```{r}
summary(baseball_sub2$BA) #checked no more NAs in BA
```
Check datatypes of all attributes
```{r}
lapply(baseball_sub2,class)
```
Datatypes for several of the columns is "chr" even though they are numeric. This will pose a problem in visualization and modeling therefore we converted the datatypes of those attributes to "integer" through a function in R using the column numbers in "baseball_sub2".
```{r}
cols = c(6,19,20,21,35,36,37,38,39,43,44)    
baseball_sub2[,cols] = apply(baseball_sub2[,cols], 2, function(x) as.integer(as.character(x)))
```
Appearances and Batting stats have been duplicated in situations where the given player played more than one position in a year. The Appearances and Batting stats show overall performance for the year, regardless of position, so we will need to remove those duplicates. 

To identify the duplicates, created two subsets of "baseball_sub2", one containing the Batting and Appearances stats ("appear_bat_sub") and the other containing the Fielding stats ("field_sub") by extracting the columns of interest by column number, while retaining yearID, playerID, teamID and lgID and including BA in each as well so that we can visualize BA against various attributes.
```{r}
appear_bat_sub <- baseball_sub2[,c(1:39,49)]
field_sub <- baseball_sub2[,c(1:4,40:49)]
names(appear_bat_sub)
names(field_sub)
```
Next determine number of dupicate rows:
```{r}
nrow(appear_bat_sub[duplicated(appear_bat_sub),])
```
Extracted distinct rows from "appear_bat_sub" in order to eliminate duplicates, creating a new subset "AppearBatSub_Clean".
```{r}
AppearBatSub_Clean <- appear_bat_sub %>% distinct
nrow(AppearBatSub_Clean)
```
We now have two clean datasets that we can use to explore the data further and
visualize.
. AppearBatSub_Clean for Appearances and Batting
. field_sub for Fielding


We now add attribute OPS which is on base percentage plus slugging average.  It reflects the ability of a player to get on base and to hit for power which some would argue is a better predictor of runs scored and wins than batting average.

To calculate OPS we need a the number of singles as well, which is not in the data, so we calculate it by subtracting all other hits from the total hits, H:
```{r}
AppearBatSub_Clean$'1B'=AppearBatSub_Clean$H-AppearBatSub_Clean$'2B'-AppearBatSub_Clean$'3B'-AppearBatSub_Clean$HR
```

Now we can calculate OPS:
```{r}
AppearBatSub_Clean$OPS <- ((AppearBatSub_Clean$H+AppearBatSub_Clean$BB+AppearBatSub_Clean$HBP)/(AppearBatSub_Clean$AB+AppearBatSub_Clean$BB+AppearBatSub_Clean$SF+AppearBatSub_Clean$HBP))+(AppearBatSub_Clean$'1B'+2*AppearBatSub_Clean$'2B'+3*AppearBatSub_Clean$`3B`+4*AppearBatSub_Clean$HR)/AppearBatSub_Clean$AB
head(AppearBatSub_Clean)
```

Simple statistics on AppearBatSub_Clean shows potential outliers:
```{r}
summary(AppearBatSub_Clean)
```
232 walks in a year (BB), 223 strikeouts (SO) and 120 intentional walks (IBB) for a given player in a season seems unusually high. Also, achieving 1.0 for batting average (BA) is anomalous. A batting average of 1 can result from getting a hit at the player's only At Bat of the season - this is not necessarily representative of good performance. We will treat the outliers in BA below.

Let's see the distribution of batting average
```{r}
boxplot(AppearBatSub_Clean$BA)

```

The box plot shows the median batting performance (BA) is around 0.2. 
```{r}
hist(AppearBatSub_Clean$BA)

```

The histogram distribution is highly skewed towards the left with a number of outliers who achieved higher than 0.4 (good batters are rare).
```{r}
summary(AppearBatSub_Clean$BA)
```

Possible outliers exist as seen by max value at 1, which is not the norm considering that most players average around .239. Outliers in BA could represent players with few at-bats, i.e. not regular players.  Let's look at the distribution of AB:
```{r}
summary(AppearBatSub_Clean$AB)
```

Distribution of AB shows the range is quite large from 1 AB to 716 in a single season. This brings into question whether we want to focus on players who have had a minimum number of At-Bats (AB) before modelling. Apart from starters, many other players make appearances throughout the season, so it would be difficult to define a minimum number of At-Bats to work with, so we decided not to invoke this condition since it would eliminate many players from our dataset.

Instead we use percentile capping on the BA outliers rather than remove them or impute to zero, because these outliers are likely natural outliers (not errors). Some players had very few At-Bats in a season so getting a couple of hits from a few At-Bats would result in a high average that's not representative of good performance.
```{r}
qnt1 <- quantile(AppearBatSub_Clean$BA, probs=c(.25, .75),na.rm=T)
caps1 <- quantile(AppearBatSub_Clean$BA, probs=c(.05, .95),na.rm=T)
H1 <- 1.5*IQR(AppearBatSub_Clean$BA, na.rm=T)
AppearBatSub_Clean$BA[AppearBatSub_Clean$BA<(qnt1[1]-H1)] <- caps1[1]
AppearBatSub_Clean$BA[AppearBatSub_Clean$BA>(qnt1[2]+H1)] <- caps1[2]
```

Check max and min after outliers capped
```{r}
summary(AppearBatSub_Clean$BA)
```

Plots after outlier capping now reflect a more normal distribution
```{r}
boxplot(AppearBatSub_Clean$BA)
hist(AppearBatSub_Clean$BA)

```

Let's explore OPS
```{r}
summary(AppearBatSub_Clean$OPS)
boxplot(AppearBatSub_Clean$OPS)
hist(AppearBatSub_Clean$OPS)
```

Histogram of OPS shows that a greater number of OPS values are between 0.5 and 1.  We will percentile cap the OPS outliers as well (like above treatment on BA outliers), since we don't want to remove the values from the dataset, but we need to change the values because the outliers will skew the data and will reduce the model's effectiveness (ex. OPS of 5 looks extreme given the best OPS in one season was 1.42177 (www.baseballreference.com)).
```{r}
qnt2 <- quantile(AppearBatSub_Clean$OPS, probs=c(.25, .75),na.rm=T)
caps2 <- quantile(AppearBatSub_Clean$OPS, probs=c(.05, .95),na.rm=T)
H2 <- 1.5*IQR(AppearBatSub_Clean$OPS, na.rm=T)
AppearBatSub_Clean$OPS[AppearBatSub_Clean$OPS<(qnt2[1]-H2)] <- caps2[1]
AppearBatSub_Clean$OPS[AppearBatSub_Clean$OPS>(qnt2[2]+H2)] <- caps2[2]

summary(AppearBatSub_Clean$OPS)
```
Let's now explore the numeric variables a bit more through a multi-plot function:
```{r}
# Get only numeric colums
list_of_numcols1 = sapply(AppearBatSub_Clean, is.numeric)
numcols1 = AppearBatSub_Clean[ , list_of_numcols1]
#Melt data --> Turn it into skinny LONG table - good to aggregate

library(reshape2)

melt_data1 = melt(numcols1, id.vars=c("yearID")) 
#This data structure is now suitable for a multiplot function
ggplot(data = melt_data1, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
```

The above histograms indicate that it is rare for players to have high values for most of the attributes as the values are skewed towards zero, consistent with previous findings.

Add binary variable for target class OPS
```{r}
AppearBatSub_Clean$OPSClass=ifelse(AppearBatSub_Clean$OPS>=.8,"good","ok")
head(AppearBatSub_Clean$OPS)
head(AppearBatSub_Clean$OPSClass)
```
Identify improved players.  We focus on the most recent 5 years.  

First, create smaller dataframes with just the 2 years of interest by filtering based on yearID.  
```{r}
mlbdata <- AppearBatSub_Clean
recent=c(2015,2016)
recent_b = c(2014, 2015)
recent_c = c(2013, 2014)
recent_d = c(2012, 2013)
recent_e = c(2011, 2012)

mlbdata1=filter(mlbdata,yearID %in% recent)

mlbdata_b=filter(mlbdata,yearID %in% recent_b)

mlbdata_c=filter(mlbdata,yearID %in% recent_c)

mlbdata_d=filter(mlbdata,yearID %in% recent_d)

mlbdata_e=filter(mlbdata,yearID %in% recent_e)
```
Next, separate the 2 years of interest into their own tables. 
```{r}
#filter out 2015 & 2016 into their own tables
firstyr = filter(mlbdata1, yearID == 2015)
secyr=filter(mlbdata1, yearID == 2016)

#filter out 2014 & 2015 into their own tables
firstyr_b = filter(mlbdata_b, yearID == 2014)
secyr_b=filter(mlbdata_b, yearID == 2015)

#filter out 2013 & 2014 into their own tables
firstyr_c = filter(mlbdata_c, yearID == 2013)
secyr_c=filter(mlbdata_c, yearID == 2014)

#filter out 2012 & 2013 into their own tables
firstyr_d = filter(mlbdata_d, yearID == 2012)
secyr_d=filter(mlbdata_d, yearID == 2013)

#filter out 2011 & 2012 into their own tables
firstyr_e = filter(mlbdata_e, yearID == 2011)
secyr_e=filter(mlbdata_e, yearID == 2012)
```

Then, join the tables so that we have each of the 2 years' data in separate columns. Left join was used because we wanted stats of players in the most recent year (ex. 2016) who existed in the previous year (ex.2015). If that condition was not met, then they did not form part of the resultant table called "new" (NAs resulted).
```{r}
#join 2015 and 2016 based on players in 2015
new=left_join(firstyr,secyr,by=c("playerID","teamID"),na=c("","NA","#NA"))
head(new)

#join 2014 and 2015 based on players in 2014
new_b=left_join(firstyr_b,secyr_b,by=c("playerID","teamID"),na=c("","NA","#NA"))
head(new_b)

#join 2013 and 2014 based on players in 2013
new_c=left_join(firstyr_c,secyr_c,by=c("playerID","teamID"),na=c("","NA","#NA"))
head(new_c)

#join 2012 and 2013 based on players in 2012
new_d=left_join(firstyr_d,secyr_d,by=c("playerID","teamID"),na=c("","NA","#NA"))
head(new_d)

#join 2011 and 2012 based on players in 2011
new_e=left_join(firstyr_e,secyr_e,by=c("playerID","teamID"),na=c("","NA","#NA"))
head(new_e)

```
Remove rows where there are NAs as these instances represent data that is not available in the later year to compare to the previous year
```{r}
new1=new[!is.na(new$yearID.y),]
summary(new1$yearID.y)

new1_b=new_b[!is.na(new_b$yearID.y),]
summary(new1_b$yearID.y) #checked no more NAs 

new1_c=new_c[!is.na(new_c$yearID.y),]
summary(new1_c$yearID.y) #checked no more NAs 

new1_d=new_d[!is.na(new_d$yearID.y),]
summary(new1_d$yearID.y) #checked no more NAs 

new1_e=new_e[!is.na(new_e$yearID.y),]
summary(new1_e$yearID.y) #checked no more NAs 

```

Add new class - whether or not improved from earlier year to the later year. Identified a new target class representing improvement from first year to second. If OPS is ok (i.e. <.800) in first year and increased to good (>=.800) in second year then the player has improved (Improved=Yes) otherwise no improvement is evident (Improved=No). Players who were already good (>= 0.800) in first year were removed from the dataset to focus on improved players.
```{r}
#add new class - whether or not improved from 2015 to 2016
new1 = filter(new1, OPS.x<0.800)
new1$Improved=ifelse(new1$OPSClass.x=="ok"&new1$OPSClass.y=="good","Yes","No")
head(new1$Improved)

#add new class - whether or not improved from 2014 to 2015
new1_b = filter(new1_b, OPS.x<0.800)
new1_b$Improved=ifelse(new1_b$OPSClass.x=="ok"&new1_b$OPSClass.y=="good","Yes","No")
head(new1_b$Improved)

#add new class - whether or not improved from 2013 to 2014
new1_c = filter(new1_c, OPS.x<0.800)
new1_c$Improved=ifelse(new1_c$OPSClass.x=="ok"&new1_c$OPSClass.y=="good","Yes","No")
head(new1_c$Improved)

#add new class - whether or not improved from 2012 to 2013
new1_d = filter(new1_d, OPS.x<0.800)
new1_d$Improved=ifelse(new1_d$OPSClass.x=="ok"&new1_d$OPSClass.y=="good","Yes","No")
head(new1_d$Improved)

#add new class - whether or not improved from 2011 to 2012
new1_e = filter(new1_e, OPS.x<0.800)
new1_e$Improved=ifelse(new1_e$OPSClass.x=="ok"&new1_e$OPSClass.y=="good","Yes","No")
head(new1_e$Improved)
```
Combine all data into one called 'total'
```{r}
total = rbind(new1, new1_b, new1_c, new1_d, new1_e)
nrow(total)
summary(total)
head(total)

# no. and proportion of improved players 2011 to 2016
freq_imp_tot=table(total$Improved)
head(freq_imp_tot)
prop.table(freq_imp_tot)
barplot(freq_imp_tot) 
```

Frequency of improved players compared to not improved shows most who were ok in the first year remained just ok in the second year.  When looking at year over year comparisons from 2011 to 2016, 1,200 of ok players (86%) did not improve in the next year, while 191 did (14%).This finding supports the notion that it is difficult to move from being an "ok"" player to "good" from one year to the next.

Decision Tree Modelling based on Improved Class

First generate train and test sets
```{r}
#split data into train and test 70-30% split
require(caTools)
set.seed(101) 
sampleimp = sample.split(total$Improved, SplitRatio = .70)
trainimp = subset(total, sampleimp == TRUE)
testimp = subset(total, sampleimp == FALSE)
```
Fit the decision tree model
```{r}
library(rpart)
(fitimp <- rpart(Improved ~R.x+RBI.x+BA.x+H.x+HR.x+IBB.x+SF.x+BB.x+AB.x+`2B.x`+`3B.x`+SB.x.x+CS.x.x+SO.x+HBP.x+SH.x+SO.x+GIDP.x,data=trainimp,method="class"))
library(rpart.plot)
library(rattle)
fancyRpartPlot(fitimp)

```

Determine confusion matrix and accuracy score on train set and then appyl model to test set and obtain accuracy

```{r}
#on training data
tree_predict=predict(fitimp,trainimp, type="class")
(conf_matrix=table(tree_predict,trainimp$Improved))

accuracy<-(conf_matrix[1,1]+conf_matrix[2,2])/(sum(conf_matrix))
print(paste('Accuracy',accuracy))

#validate on test data
validpred=predict(fitimp,testimp,type="class")
conf_matrix_val<-table(validpred,testimp$Improved)
conf_matrix_val
accuracy_val<-(conf_matrix_val[1,1]+conf_matrix_val[2,2])/(sum(conf_matrix_val))
print(paste('Accuracy',accuracy_val))
```
Accuracy went down from 88% on training data to 84% on test data.  On test data, the model predicted correctly 350 out of 360 instances for class NO, whereas only 1 out of 57 cases of class YES was predicted correctly. This is due to the training dataset being highly imbalanced. Although overall accuracy seems high (~85%), the prediction accuracy for
class YES, which is the class of interest, needs improvement. The class NO is overrepresented in the training set. The algorithm that learns from this imbalanced training set would likely not perform well on examples in class YES as it did not receive enough information about the minority class YES when being trained.

To improve the fitted model, undersampling of majority class NO and oversampling of minority class YES will be used by applying the ROSE function on the training data. ROSE package provides functions to deal with binary classification problems when there are imbalanced classes like we have here where instances in class YES (improved players from one year to the next based on OPS) is rare.

Select attributes from the larger dataframe that could be potential predictors for the model
```{r}
trainimp = trainimp[c("R.x", "RBI.x", "H.x", "HR.x","IBB.x", "SF.x","BB.x", "AB.x","BA.x","SB.x.x","CS.x.x","SO.x","HBP.x","SH.x","GIDP.x","Improved")]
testimp = testimp[c("R.x", "RBI.x", "H.x", "HR.x","IBB.x", "SF.x","BB.x", "AB.x","BA.x","SB.x.x","CS.x.x","SO.x","HBP.x","SH.x","GIDP.x","Improved")]

```
Apply ROSE package
```{r}
library(ROSE)
data_balanced_rose <- ROSE(Improved ~ .,data = trainimp,seed = 1)$data
table(data_balanced_rose$Improved)
prop.table(table(data_balanced_rose$Improved))
nrow(data_balanced_rose)
ncol(data_balanced_rose)
```

After applying ROSE, a more balanced training dataset is created, which contains 470 instances in YES and 504 in NO (total 974 samples which is 70% of the original dataset containing 1,391 instances).

Let's fit the model using the balanced data
```{r}
library(rpart)
(fitimp_bal <- rpart(Improved ~R.x+RBI.x+BA.x+H.x+HR.x+IBB.x+SF.x+BB.x+AB.x+SB.x.x+CS.x.x+SO.x+HBP.x+SH.x+SO.x+GIDP.x,data=data_balanced_rose,method="class"))
library(rpart.plot)
library(rattle)
fancyRpartPlot(fitimp_bal)

```


